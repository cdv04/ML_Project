---
title: "Machine Learning Project"
author: "Claire Della Vedova"
date: "10 août 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warnings=FALSE)
```


##1. Background


Data used in this project come from accelerometers on the belt, forearm, arm, and dumbell.

Six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways : 

A - exactly according to the specification
B - throwing the elbows to the front
C - lifting the dumbbell only halfway
D - lowering the dumbbell only halfway
E - throwing the hips to the front



Using the data of these 6 participants **the aim of this Machine Learning project is to train a model to predict the manner in which 20 new participants did the exercise**. 



![](img/sensors.jpg)

*From : Velloso, Eduardo, et al. "Qualitative activity recognition of weight lifting exercises." Proceedings of the 4th Augmented Human International Conference. ACM, 2013.*


*NB : More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).*

## Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 


```{r}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")

testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

training[1:10,1:10]
```



##2. Packages needed

```{r package}

library(tidyverse)
library(caret)
library(funModeling)

```

##1. First look on data

###1.1 Dimensions
```{r}
dim(training)
```

Training data contains 159 possible explanatory variables, and 196622 observations ! It will be impossible (beacause of time computation) to use all the available data to train the model.So latter in the project we'are going to use a little part of these data.

###1.2 Structure
```{r}
str(training)
```

Some variables seem to be factor by error and contains #DIV/0!". I'm going to remove these factor variables.



##3. Explanatory variables selection


###3.1 Remonving false factor variables

```{r}
# data set of exmplanatory variables (response variable is removed)
Descr_Var <-training %>%
	select(-classe)

# in this new data set, false factor variables are removed
Descr_Var2 <- Descr_Var %>%
	dplyr::select_if(~!is.factor(.x))

```



###3.2 Removing variables with more than 10% of NA values


Using the df_status function of the funModeling package, we can see that some variables contain a high percentage of missing values. I'm going to remove all the explanatory variables having more than 10% of NA values using code available in *Casas, Pablo. Data Science Live Book: An intuitive and practical approach to data analysis, data preparation and machine learning, suitable for all ages! (p. 13). Édition du Kindle*.   

```{r}
Descr_Var2_status <- df_status(Descr_Var2)
 
```

```{r}
# Removing variables with at least 10% of na values 
vars_to_remove = filter( Descr_Var2_status, p_na >= 10) %>%
	.$ variable 

vars_to_remove

```

```{r}
# Keeping all columns except the ones present in 'vars_to_remove' vector 
Descr_Var2 = select(Descr_Var2, -one_of( vars_to_remove))

```



###3.3 Removing variables having Zero and near zero-variance Predictors:

To remove the explanatory variables having Zero or near zero-variance, I use the "nzv"" function of "caret" package

```{r}
nzv <- nearZeroVar(Descr_Var2, saveMetrics= TRUE)
nzv

```

There are no explanatory variables having Zero or near zero-variance

```{r}
dim(Descr_Var2)
```

Now there are 56 possible predictors variables.

###3.4 Removing other variables

####3.4.1 X variable

X variable has no information, is row index, so I'm going to remove it.

```{r}
Descr_Var2 <- Descr_Var2 %>%
	select(-"X")
```


####3.4.2 Date and Time variables

Date and Time variables have no usable informations.So, I'm going to remove theù too.
```{r}

Descr_Var2 <- Descr_Var2 %>%
	select(-matches("times"))

```

####3.4.3 num_window variable

As the same for the "num_window" variable.
```{r}
Descr_Var2 <- Descr_Var2 %>%
	select(-"num_window")
```


###3.5 Removing high correlated predictors (cutoff used= 0.75)
```{r}


Descr_Var2_Cor <-  cor(Descr_Var2)
highlyCorDescr <- findCorrelation(Descr_Var2_Cor, cutoff = .75)

Descr_Var3 <- Descr_Var2[,-highlyCorDescr]

```



###3.6 Removing Linear combination

```{r}
comboInfo <- findLinearCombos(Descr_Var3)
comboInfo 
```

No linear combination, so no variable to remove



```{r}
ncol(Descr_Var3)
```

At the end of this step of variable selection, 34 are still usable to train a model to predict the manner in which new 20 participants did the exercice.

##4. Split training into training and valid sets

###4.1 Applying variable selection on training

```{r}
selected_variables <- names(Descr_Var3)

# adding classe Variable
selected_variables <- c( selected_variables, names(training)[ncol(training)])

```


```{r}

training2 <- training %>%
	select(one_of(selected_variables))


```

###4.2 Split training into train_dat and valid_data sets

```{r}
p_init <- 0.25

set.seed(1234)
indTmp <- createDataPartition(y=training$classe, p=p_init, list=FALSE)
tmp_data <- training2[indTmp,]

set.seed(1234)
indTrain <- createDataPartition(y=tmp_data$classe, p=0.75, list=FALSE)
train_data <- tmp_data[indTrain,]
valid_data <- tmp_data[-indTrain,]
```

##5. Train Models

###5.1 Random Forest

```{r, cache=TRUE}
mod_rf <- train(classe~., data=train_data, method="rf", prox=TRUE)
pred_rf <- predict(mod_rf, valid_data)
CM_rf <- confusionMatrix(valid_data$classe, pred_rf)
CM_rf
```

###5.2 Boosting with tree

```{r,cache=TRUE}
mod_gbm <- train(classe~., data=train_data, method="gbm")
pred_gbm <- predict(mod_gbm, valid_data)
CM_gbm <- confusionMatrix(valid_data$classe, pred_gbm)
CM_gbm 
```


##5.4 gbm + rf

```{r,cache=TRUE}
predDF <- data.frame(pred_rf, pred_gbm, classe=valid_data$classe)
mod_comb <- train(classe~.,method="treebag", data=predDF)
pred_comb <- predict(mod_comb, valid_data)
CM_comb <- confusionMatrix(valid_data$classe, pred_comb)
CM_comb
```



