---
title: "Machine Learning Project"
author: "Claire Della Vedova"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```




##1. Background


Data used in this project come from accelerometers on the belt, forearm, arm, and dumbell.

Six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways : 

+ A - exactly according to the specification
+ B - throwing the elbows to the front
+ C - lifting the dumbbell only halfway
+ D - lowering the dumbbell only halfway
+ E - throwing the hips to the front



Using the data of these 6 participants **the aim of this Machine Learning project is to train a model to predict the manner in which 20 new participants did the exercise**. 



![](img/sensors.jpg)

*From : Velloso, Eduardo, et al. "Qualitative activity recognition of weight lifting exercises." Proceedings of the 4th Augmented Human International Conference. ACM, 2013.*


*NB : More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).*

##2. Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 


```{r, cache=TRUE}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")

testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

```



##3. Packages needed

```{r package}

library(tidyverse)
library(caret)
library(funModeling)
library(AppliedPredictiveModeling)

```

##4. First look on data

###4.1 Dimensions
```{r}
dim(training)
```

Training data contains 159 possible explanatory variables, and 19622 observations ! It will be impossible (beacause of time computation) to use all the available data to train the model.So latter in the project we'are going to use a little part of these data.

###4.2 Structure
```{r}
str(training)
```

Some variables seem to be factor by error and contains #DIV/0!". I'm going to remove these factor variables.

###4.3 Distribution of classe modalities

```{r}
freq(data=training, input=c("classe"))
```


##5. Explanatory variables selection


###5.1 Remonving false factor variables

```{r}
# data set of exmplanatory variables (response variable is removed)
Descr_Var <-training %>%
	select(-classe)

# in this new data set, false factor variables are removed
Descr_Var2 <- Descr_Var %>%
	dplyr::select_if(~!is.factor(.x))

```



###5.2 Removing variables with more than 10% of NA values


Using the df_status function of the funModeling package, we can see that some variables contain a high percentage of missing values. I'm going to remove all the explanatory variables having more than 10% of NA values using code available in *Casas, Pablo. Data Science Live Book: An intuitive and practical approach to data analysis, data preparation and machine learning, suitable for all ages! (p. 13). Édition du Kindle*.   

```{r}
Descr_Var2_status <- df_status(Descr_Var2)
 
```

```{r}
# Removing variables with at least 10% of na values 
vars_to_remove = filter( Descr_Var2_status, p_na >= 10) %>%
	.$ variable 

vars_to_remove

```

```{r}
# Keeping all columns except the ones present in 'vars_to_remove' vector 
Descr_Var2 = select(Descr_Var2, -one_of( vars_to_remove))

```



###5.3 Removing variables having Zero and near zero-variance Predictors:

To remove the explanatory variables having Zero or near zero-variance, I use the "nzv"" function of "caret" package

```{r}
nzv <- nearZeroVar(Descr_Var2, saveMetrics= TRUE)
nzv

```

There are no explanatory variables having Zero or near zero-variance

```{r}
dim(Descr_Var2)
```

Now there are 56 possible predictors variables.

###5.4 Removing other variables

####5.4.1 X variable

X variable has no information, is row index, so I'm going to remove it.

```{r}
Descr_Var2 <- Descr_Var2 %>%
	select(-"X")
```


####5.4.2 Date and Time variables

Date and Time variables have no usable informations.So, I'm going to remove theù too.
```{r}

Descr_Var2 <- Descr_Var2 %>%
	select(-matches("times"))

```

####5.4.3 num_window variable


As the same for the "num_window" variable.
```{r}
Descr_Var2 <- Descr_Var2 %>%
	select(-"num_window")
```


###5.5 Removing high correlated predictors (cutoff used= 0.75)
```{r}


Descr_Var2_Cor <-  cor(Descr_Var2)
highlyCorDescr <- findCorrelation(Descr_Var2_Cor, cutoff = .75)

Descr_Var3 <- Descr_Var2[,-highlyCorDescr]

```



###5.6 Removing Linear combination

```{r}
comboInfo <- findLinearCombos(Descr_Var3)
comboInfo 
```

No linear combination, so no variable to remove



```{r}
ncol(Descr_Var3)
```

At the end of this step of variable selection, 34 are still usable to train a model to predict the manner in which new 20 participants did the exercice.




##6. Split training into training and valid sets

###6.1 Applying variable selection on training

```{r}
selected_variables <- names(Descr_Var3)

# adding classe Variable
selected_variables <- c( selected_variables, names(training)[ncol(training)])

```


```{r}

training2 <- training %>%
	select(one_of(selected_variables))


```

###6.2 Split training into train_dat and valid_data sets

Since the training data set contains too much lines to train the model (because of calculus time), we need to select a little part of them. I choose to use 25% of the 19622 observations, i.e 4907 observations. And, also I limit the explanatory variable to those previously selected.

```{r}
p_init <- 0.25

set.seed(1234)
indTmp <- createDataPartition(y=training2$classe, p=p_init, list=FALSE)
tmp_data <- training2[indTmp,]
```

```{r}
dim(tmp_data)
```

Then, I split this part of training data into train and valid datasets. The train part will be used to train machine learning models, and the valid dataset will be used to assess their performance. 


```{r}
set.seed(1234)
indTrain <- createDataPartition(y=tmp_data$classe, p=0.75, list=FALSE)
train_data <- tmp_data[indTrain,]
valid_data <- tmp_data[-indTrain,]
```

The train data contains 75% of the kept rows, i.e 3682 rows. The valid dataset contains 1225 rows.
```{r}
dim(train_data)
dim(valid_data)

```



##7. Train Models

Since random forests are known to have good performance in classification machine learning 
problems, I chose to use this kind of model.

###7.1 Random Forest

```{r, cache=TRUE}
mod_rf <- train(classe~., data=train_data, method="rf", prox=TRUE)
pred_rf <- predict(mod_rf, valid_data)
CM_rf <- confusionMatrix(valid_data$classe, pred_rf)
CM_rf
```

```{r}
eose_rf <- round(CM_rf$overall[1],3)
```

**The expected out of sample error is `r eose_rf`.**

&nbsp;

```{r}
plot(CM_rf$table, color=c("#FF7F00", "#00008B", "#0000EE", "#4876FF", "#00BFFF"), main="Random Forest")

```


###7.2 Boosting with tree

As a second kind a model, I chose to use a Generalized Boosted Model because it's also known as having good performance in classification.

```{r,cache=TRUE}
mod_gbm <- train(classe~., data=train_data, method="gbm", verbose = FALSE)
pred_gbm <- predict(mod_gbm, valid_data)
CM_gbm <- confusionMatrix(valid_data$classe, pred_gbm)
CM_gbm 
```

```{r}
eose_gbm <- round(CM_gbm$overall[1],3)

```

**The expected out of sample error is `r eose_gbm`.**

&nbsp;

```{r}
plot(CM_gbm$table, color=c("#FF7F00", "#00008B", "#0000EE", "#4876FF", "#00BFFF"), main="Generalized Boosted Model")

```

We can see tah erros of prediction are not the same of the random forets model.

&nbsp;

###7.3 Ensemble model :  gbm + rf

At a final step, I use an esemble model combinig predictions of the random forest and the generalized boostring models, using a tree bag approach. Predictions of ensemble model are often better than predictions of a single model.

```{r,cache=TRUE}
predDF <- data.frame(pred_rf, pred_gbm, classe=valid_data$classe)
mod_comb <- train(classe~.,method="treebag", data=predDF)
pred_comb <- predict(mod_comb, valid_data)
CM_comb <- confusionMatrix(valid_data$classe, pred_comb)
CM_comb
```



```{r}
eose_comb <- round(CM_comb$overall[1],3)

```

**The expected out of sample error is `r eose_comb`**. The accuracy of the ensemble model is the same as the one of the random forest model.


```{r}
plot(CM_comb$table, color=c("#FF7F00", "#00008B", "#0000EE", "#4876FF", "#00BFFF"), main="Generalized Boosted Model")
```



##8. Prediction of the 20 test cases

Since the random forest has the same accuracy of the ensemble model, I'm going to use the random forest model to predict the 20 test cases


```{r}

predict(mod_rf, testing)

```

